{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6ae462-832f-484f-bb52-3599b3bf4f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a857d2-15a6-4572-87b6-437f860c5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705e296b-3e4f-4d18-a12e-3296dd3dcfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.distributions.Normal(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9912f5ad-5c2e-4814-b440-786f0469479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0532)\n"
     ]
    }
   ],
   "source": [
    "z = q.rsample()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18720596-d80e-4b9c-ac79-74b22825aadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob pz:  tensor(-19.2396) prob: tensor(4.4089e-09)\n",
      "log prob qzx:  tensor(-2.8186) prob: tensor(0.0597)\n"
     ]
    }
   ],
   "source": [
    "p = torch.distributions.Normal(0, 1)\n",
    "q = torch.distributions.Normal(2, 4)\n",
    "\n",
    "log_pz = p.log_prob(z)\n",
    "log_qzx = q.log_prob(z)\n",
    "\n",
    "print('log prob pz: ', log_pz, 'prob:', torch.exp(log_pz))\n",
    "print('log prob qzx: ', log_qzx, 'prob:', torch.exp(log_qzx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f99d480-6ac2-47e4-8e2c-13dce7b7cc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.4210)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence = log_qzx - log_pz\n",
    "kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04dd62d-a74a-405c-94d7-92e3ad9a2584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob pz:  tensor(-19.2396) prob: tensor(4.4089e-09)\n",
      "log prob qzx:  tensor(-19.2396) prob: tensor(4.4089e-09)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.distributions.Normal(0, 1)\n",
    "\n",
    "log_pz = p.log_prob(z)\n",
    "log_qzx = q.log_prob(z)\n",
    "\n",
    "print('log prob pz: ', log_pz, 'prob:', torch.exp(log_pz))\n",
    "print('log prob qzx: ', log_qzx, 'prob:', torch.exp(log_qzx))\n",
    "kl_divergence = log_qzx - log_pz\n",
    "kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e7f1757-1f8c-4b69-bc8e-2ce80aa0cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1, z2 = q.rsample(), q.rsample()\n",
    "y = z1 + z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0dc1465-0ea3-4a3f-8c44-733bcb4e8366",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c06015ea-3a0f-461d-9468-a79d001e9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/pl_bolts/utils/warnings.py:30: UserWarning: You want to use `wandb` which is not installed yet, install it with `pip install wandb`.\n",
      "  stdout_func(\n",
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/pl_bolts/utils/warnings.py:30: UserWarning: You want to use `gym` which is not installed yet, install it with `pip install gym`.\n",
      "  stdout_func(\n",
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/pl_bolts/utils/warnings.py:30: UserWarning: You want to use `sklearn` which is not installed yet, install it with `pip install sklearn`.\n",
      "  stdout_func(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pl_bolts.datamodules import CIFAR10DataModule, ImagenetDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ec9ef4f-2dea-42ec-bdd7-1b58ee200ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    ")\n",
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # encoder, decoder\n",
    "        self.encoder = resnet18_encoder(False, False)\n",
    "        self.decoder = resnet18_decoder(\n",
    "            latent_dim=latent_dim, \n",
    "            input_height=input_height, \n",
    "            first_conv=False, \n",
    "            maxpool1=False\n",
    "        )\n",
    "\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim, latent_dim)\n",
    "\n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def gaussian_likelihood(self, mean, logscale, sample):\n",
    "        scale = torch.exp(logscale)\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "        log_pxz = dist.log_prob(sample)\n",
    "        return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        # decoded \n",
    "        x_hat = vae.decoder(z)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "\n",
    "        # kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'kl': kl.mean(),\n",
    "            'recon_loss': recon_loss.mean(), \n",
    "            'reconstruction': recon_loss.mean(),\n",
    "            'kl': kl.mean(),\n",
    "        })\n",
    "\n",
    "        return elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9981eef2-57d4-47ab-b50d-59a56e25450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "\n",
    "datamodule = CIFAR10DataModule('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62da9f-74a6-466c-94e6-7ca1a7d0e657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "970e83ff-f11a-4a53-9202-8b294f39bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [03:57<00:00, 717594.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to .\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /Users/Egor_Osinkin/projects/vae4fashion/lightning_logs\n",
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:88: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:107: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | encoder | ResNetEncoder | 11.2 M\n",
      "1 | decoder | ResNetDecoder | 8.6 M \n",
      "2 | fc_mu   | Linear        | 131 K \n",
      "3 | fc_var  | Linear        | 131 K \n",
      "------------------------------------------\n",
      "20.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.1 M    Total params\n",
      "80.228    Total estimated model params size (MB)\n",
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|██▎                                                                                               | 30/1250 [02:11<1:28:53,  4.37s/it, loss=3.08e+03, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Egor_Osinkin/Library/Caches/pypoetry/virtualenvs/vae4fashion-tiTJ48CW-py3.9/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(1234)\n",
    "\n",
    "vae = VAE()\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=30, progress_bar_refresh_rate=10, accelerator='cpu')\n",
    "trainer.fit(vae, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59210eec-9a5a-44ad-b518-26784242064b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
